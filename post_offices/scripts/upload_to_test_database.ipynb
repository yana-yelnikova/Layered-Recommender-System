{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af1d231e",
   "metadata": {},
   "source": [
    "Connecting to the Database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f9e87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy as sa\n",
    "import io\n",
    "\n",
    "# Create the connection string with placeholders for credentials\n",
    "db_uri = \"postgresql+psycopg2://<USERNAME>:<PASSWORD>@ep-falling-glitter-a5m0j5gk-pooler.us-east-2.aws.neon.tech:5432/neondb?sslmode=require\"\n",
    "\n",
    "# Create the Engine object\n",
    "engine = sa.create_engine(db_uri, pool_pre_ping=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19447d80",
   "metadata": {},
   "source": [
    "Checking existing schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "558a57f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available schemas in the database:\n",
      "['dependency_example', 'information_schema', 'nyc_schools', 'public', 'test_berlin_data']\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import inspect\n",
    "# Create an inspector object from engine\n",
    "inspector = inspect(engine)\n",
    "\n",
    "# Get the list of schema names\n",
    "schemas = inspector.get_schema_names()\n",
    "\n",
    "print(\"Available schemas in the database:\")\n",
    "print(schemas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3416e431",
   "metadata": {},
   "source": [
    "Creating a table 'post_offices_test' in 'test_berlin_data' schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5aa13b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'post_offices_test' created successfully using the SQLAlchemy engine.\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import text\n",
    "\n",
    "# SQL statement to create the table\n",
    "create_table_sql = \"\"\"\n",
    "DROP TABLE IF EXISTS test_berlin_data.post_offices_test;\n",
    "CREATE TABLE test_berlin_data.post_offices_test (\n",
    "    id VARCHAR(20) PRIMARY KEY,\n",
    "    district_id VARCHAR(20),\n",
    "    neighborhood_id VARCHAR(20),\n",
    "    zip_code VARCHAR(10),\n",
    "    city VARCHAR(20),\n",
    "    street VARCHAR(200),\n",
    "    house_no VARCHAR(20),\n",
    "    location_type VARCHAR(200),\n",
    "    location_name VARCHAR(200),\n",
    "    closure_periods VARCHAR(400),\n",
    "    opening_hours VARCHAR(400),\n",
    "    latitude DECIMAL(9,6),\n",
    "    longitude DECIMAL(9,6)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Connect using the engine and execute the SQL\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        connection.execute(text(create_table_sql))\n",
    "        connection.commit() # Commit the transaction to make the change permanent\n",
    "    print(\"Table 'post_offices_test' created successfully using the SQLAlchemy engine.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during table creation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeca084",
   "metadata": {},
   "source": [
    "Preparing the data for upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8d60021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame has been prepared for upload with the correct column order.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 244 entries, 0 to 243\n",
      "Data columns (total 13 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   id               244 non-null    int64  \n",
      " 1   district_id      244 non-null    int64  \n",
      " 2   neighborhood_id  244 non-null    int64  \n",
      " 3   zip_code         244 non-null    int64  \n",
      " 4   city             244 non-null    object \n",
      " 5   street           244 non-null    object \n",
      " 6   house_no         244 non-null    object \n",
      " 7   location_type    244 non-null    object \n",
      " 8   location_name    234 non-null    object \n",
      " 9   closure_periods  244 non-null    object \n",
      " 10  opening_hours    244 non-null    object \n",
      " 11  latitude         244 non-null    float64\n",
      " 12  longitude        244 non-null    float64\n",
      "dtypes: float64(2), int64(4), object(7)\n",
      "memory usage: 24.9+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import io\n",
    "\n",
    "# --- 1. Load the CSV file ---\n",
    "file_to_load = Path('../clean/deutschepost_clean_with_distr.csv')\n",
    "df = pd.read_csv(file_to_load)\n",
    "\n",
    "# --- 2. Define the correct column order based on NEW CREATE TABLE statement ---\n",
    "sql_column_order = [\n",
    "    'id',\n",
    "    'district_id',\n",
    "    'neighborhood_id',\n",
    "    'zip_code',\n",
    "    'city',\n",
    "    'street',\n",
    "    'house_no',\n",
    "    'location_type',\n",
    "    'location_name',\n",
    "    'closure_periods',\n",
    "    'opening_hours',\n",
    "    'latitude',\n",
    "    'longitude'\n",
    "]\n",
    "\n",
    "# --- 3. Create the final DataFrame for upload with columns in the correct order ---\n",
    "df_for_upload = df[sql_column_order]\n",
    "\n",
    "print(\"DataFrame has been prepared for upload with the correct column order.\")\n",
    "df_for_upload.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85f6a5f",
   "metadata": {},
   "source": [
    "Insert Data into Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b7cefd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Transaction committed successfully. 244 rows were copied.\n"
     ]
    }
   ],
   "source": [
    "raw_conn = None\n",
    "try:\n",
    "    # Get a single, low-level connection from the engine\n",
    "    raw_conn = engine.raw_connection()\n",
    "    cursor = raw_conn.cursor()\n",
    "\n",
    "    # Set the search path for this session\n",
    "    cursor.execute(\"SET search_path TO test_berlin_data;\")\n",
    "    \n",
    "    # --- Load data into the table ---\n",
    "    # We need to save to the buffer WITH a header for the COPY CSV command to work correctly\n",
    "    buffer = io.StringIO()\n",
    "    df_for_upload.to_csv(buffer, index=False, header=True) # Note: header=True\n",
    "    buffer.seek(0)\n",
    "    \n",
    "    # Use copy_expert with CSV format to correctly handle commas in data\n",
    "    copy_sql = \"\"\"\n",
    "        COPY post_offices_test FROM STDIN WITH\n",
    "            (FORMAT CSV, HEADER TRUE)\n",
    "    \"\"\"\n",
    "    cursor.copy_expert(sql=copy_sql, file=buffer)\n",
    "    \n",
    "    # Commit the entire transaction\n",
    "    raw_conn.commit()\n",
    "    print(f\"✅ Transaction committed successfully. {len(df_for_upload)} rows were copied.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ An error occurred: {e}\")\n",
    "    if raw_conn:\n",
    "        raw_conn.rollback()\n",
    "finally:\n",
    "    if raw_conn:\n",
    "        raw_conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffae20d",
   "metadata": {},
   "source": [
    "Adding the Foreign Key Constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ec17d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Foreign key constraint 'district_id_fk' added successfully.\n"
     ]
    }
   ],
   "source": [
    "# SQL statement to add the foreign key constraint\n",
    "add_foreign_key_sql = \"\"\"\n",
    "ALTER TABLE test_berlin_data.post_offices_test\n",
    "ADD CONSTRAINT district_id_fk FOREIGN KEY (district_id)\n",
    "REFERENCES test_berlin_data.districts(district_id)\n",
    "ON DELETE RESTRICT\n",
    "ON UPDATE CASCADE;\n",
    "\"\"\"\n",
    "\n",
    "# Connect using the engine and execute the SQL\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        connection.execute(text(add_foreign_key_sql))\n",
    "        connection.commit()\n",
    "    print(\"✅ Foreign key constraint 'district_id_fk' added successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while adding the foreign key: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65f65a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Verification: First 5 rows from the database ---\n",
      "        id district_id neighborhood_id zip_code    city                street  \\\n",
      "0  4340626    11001001             101    10178  Berlin        Spandauer Str.   \n",
      "1     6730    11001001             101    10178  Berlin           Rathausstr.   \n",
      "2  4307374    11001001             101    10178  Berlin  Karl-Liebknecht-Str.   \n",
      "3  4125530    11001001             101    10179  Berlin            Grunerstr.   \n",
      "4  4326999    11001001             101    10179  Berlin           Brückenstr.   \n",
      "\n",
      "  house_no            location_type                        location_name  \\\n",
      "0        2            RETAIL_OUTLET                            City Shop   \n",
      "1        5  POSTBANK_FINANCE_CENTER                     Postbank Filiale   \n",
      "2       13            RETAIL_OUTLET                     Lotto Post Tabak   \n",
      "3       20            RETAIL_OUTLET  GECO im ALEXA, Untergeschoss/Baseme   \n",
      "4       1a            RETAIL_OUTLET              Lotto-Post-Schreibwaren   \n",
      "\n",
      "                                     closure_periods  \\\n",
      "0                                                 []   \n",
      "1  [{'type': 'closure', 'fromDate': '2025-10-29T0...   \n",
      "2                                                 []   \n",
      "3                                                 []   \n",
      "4                                                 []   \n",
      "\n",
      "                                       opening_hours   latitude  longitude  \n",
      "0  Mo: 08:00-18:00; Tu: 08:00-18:00; We: 08:00-18...  52.521145  13.403767  \n",
      "1  Mo: 09:30-18:30; Tu: 09:30-18:30; We: 09:30-18...  52.519737  13.411517  \n",
      "2  Mo: 08:00-19:00; Tu: 08:00-19:00; We: 08:00-19...  52.522327  13.408074  \n",
      "3  Mo: 09:00-19:45; Tu: 09:00-19:45; We: 09:00-19...  52.518764  13.416384  \n",
      "4  Mo: 09:00-19:00; Tu: 09:00-19:00; We: 09:00-19...  52.511505  13.416914  \n"
     ]
    }
   ],
   "source": [
    "# Final check: read the first 5 rows from the new table\n",
    "try:\n",
    "    check_df = pd.read_sql(\"SELECT * FROM test_berlin_data.post_offices_test LIMIT 5\", engine)\n",
    "    print(\"--- Verification: First 5 rows from the database ---\")\n",
    "    print(check_df)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during verification: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ed3551",
   "metadata": {},
   "source": [
    "Final Validation via SQL\n",
    "\n",
    "This section executes several SQL queries directly against the database to perform final validation on the newly loaded post_offices_test table. These checks verify the total row count, ensure all coordinates fall within the expected geographical boundaries, and confirm the referential integrity of the district_id by comparing the IDs in the main table against the reference districts table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "351c1ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Total row count in 'post_offices_test' ---\n",
      "   total_rows\n",
      "0         244\n"
     ]
    }
   ],
   "source": [
    "# --- Query 1: Total row count ---\n",
    "query1 = \"SELECT COUNT(*) AS total_rows FROM test_berlin_data.post_offices_test;\"\n",
    "df1 = pd.read_sql(query1, engine)\n",
    "print(\"--- 1. Total row count in 'post_offices_test' ---\")\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74036630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2. Count of locations outside Berlin's bounding box ---\n",
      "   outliers\n",
      "0         0\n"
     ]
    }
   ],
   "source": [
    "# --- Query 2: Count of locations with coordinates outside Berlin ---\n",
    "query2 = \"\"\"\n",
    "    SELECT COUNT(*) AS outliers\n",
    "    FROM test_berlin_data.post_offices_test\n",
    "    WHERE NOT (latitude BETWEEN 52.3 AND 52.7 AND longitude BETWEEN 13.0 AND 13.8);\n",
    "\"\"\"\n",
    "df2 = pd.read_sql(query2, engine)\n",
    "print(\"\\n--- 2. Count of locations outside Berlin's bounding box ---\")\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2386e791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3. Distinct district_id's found in the post_offices_test table ---\n",
      "   district_id\n",
      "0     11001001\n",
      "1     11002002\n",
      "2     11003003\n",
      "3     11004004\n",
      "4     11005005\n",
      "5     11006006\n",
      "6     11007007\n",
      "7     11008008\n",
      "8     11009009\n",
      "9     11010010\n",
      "10    11011011\n",
      "11    11012012\n"
     ]
    }
   ],
   "source": [
    "# --- Query 3: Distinct district_id's in the post_offices table ---\n",
    "query3 = \"SELECT DISTINCT district_id FROM test_berlin_data.post_offices_test ORDER BY 1;\"\n",
    "df3 = pd.read_sql(query3, engine)\n",
    "print(\"\\n--- 3. Distinct district_id's found in the post_offices_test table ---\")\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d479c1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 4. Distinct district_id's from the reference 'districts' table ---\n",
      "   district_id\n",
      "0     11001001\n",
      "1     11002002\n",
      "2     11003003\n",
      "3     11004004\n",
      "4     11005005\n",
      "5     11006006\n",
      "6     11007007\n",
      "7     11008008\n",
      "8     11009009\n",
      "9     11010010\n",
      "10    11011011\n",
      "11    11012012\n"
     ]
    }
   ],
   "source": [
    "# --- Query 4: Distinct district_id's in the districts lookup table ---\n",
    "query4 = \"SELECT DISTINCT district_id FROM test_berlin_data.districts ORDER BY 1;\"\n",
    "df4 = pd.read_sql(query4, engine)\n",
    "print(\"\\n--- 4. Distinct district_id's from the reference 'districts' table ---\")\n",
    "print(df4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
