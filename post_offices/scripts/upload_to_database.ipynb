{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c37d084c",
   "metadata": {},
   "source": [
    "Connecting to the Database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dd72a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connection engine for the local 'layereddb' is ready.\n"
     ]
    }
   ],
   "source": [
    "import sqlalchemy as sa\n",
    "\n",
    "# Create the connection string with placeholders for credentials\n",
    "db_uri = \"postgresql+psycopg2://<USERNAME>:<PASSWORD>@localhost:5433/layereddb\"\n",
    "\n",
    "# Create the Engine object, keeping pool_pre_ping for reliability\n",
    "engine = sa.create_engine(db_uri, pool_pre_ping=True)\n",
    "\n",
    "print(\"✅ Connection engine for the local 'layereddb' is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd0e291",
   "metadata": {},
   "source": [
    "Checking existing schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc6053c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available schemas in the database:\n",
      "['berlin_labels', 'berlin_recommender', 'berlin_source_data', 'dashboard_data', 'information_schema', 'public']\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import inspect\n",
    "# Create an inspector object from engine\n",
    "inspector = inspect(engine)\n",
    "\n",
    "# Get the list of schema names\n",
    "schemas = inspector.get_schema_names()\n",
    "\n",
    "print(\"Available schemas in the database:\")\n",
    "print(schemas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6ef323",
   "metadata": {},
   "source": [
    "Creating a table 'post_offices' in 'berlin_source_data' schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "feab4a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'post_offices' created successfully using the SQLAlchemy engine.\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import text\n",
    "\n",
    "# SQL statement to create the table\n",
    "create_table_sql = \"\"\"\n",
    "DROP TABLE IF EXISTS berlin_source_data.post_offices;\n",
    "CREATE TABLE berlin_source_data.post_offices (\n",
    "    id VARCHAR(20) PRIMARY KEY,\n",
    "    district_id VARCHAR(20),\n",
    "    neighborhood_id VARCHAR(20),\n",
    "    zip_code VARCHAR(10),\n",
    "    city VARCHAR(20),\n",
    "    street VARCHAR(200),\n",
    "    house_no VARCHAR(20),\n",
    "    location_type VARCHAR(200),\n",
    "    location_name VARCHAR(200),\n",
    "    closure_periods VARCHAR(400),\n",
    "    opening_hours VARCHAR(400),\n",
    "    latitude DECIMAL(9,6),\n",
    "    longitude DECIMAL(9,6)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Connect using the engine and execute the SQL\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        connection.execute(text(create_table_sql))\n",
    "        connection.commit() # Commit the transaction to make the change permanent\n",
    "    print(\"Table 'post_offices' created successfully using the SQLAlchemy engine.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during table creation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7091ad4e",
   "metadata": {},
   "source": [
    "Preparing the data for upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a5a4a6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame has been prepared for upload with the correct column order.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 244 entries, 0 to 243\n",
      "Data columns (total 13 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   id               244 non-null    int64  \n",
      " 1   district_id      244 non-null    int64  \n",
      " 2   neighborhood_id  244 non-null    int64  \n",
      " 3   zip_code         244 non-null    int64  \n",
      " 4   city             244 non-null    object \n",
      " 5   street           244 non-null    object \n",
      " 6   house_no         244 non-null    object \n",
      " 7   location_type    244 non-null    object \n",
      " 8   location_name    234 non-null    object \n",
      " 9   closure_periods  244 non-null    object \n",
      " 10  opening_hours    244 non-null    object \n",
      " 11  latitude         244 non-null    float64\n",
      " 12  longitude        244 non-null    float64\n",
      "dtypes: float64(2), int64(4), object(7)\n",
      "memory usage: 24.9+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import io\n",
    "\n",
    "# --- 1. Load the CSV file ---\n",
    "file_to_load = Path('../clean/deutschepost_clean_with_distr.csv')\n",
    "df = pd.read_csv(file_to_load)\n",
    "\n",
    "# --- 2. Define the correct column order based on NEW CREATE TABLE statement ---\n",
    "sql_column_order = [\n",
    "    'id',\n",
    "    'district_id',\n",
    "    'neighborhood_id',\n",
    "    'zip_code',\n",
    "    'city',\n",
    "    'street',\n",
    "    'house_no',\n",
    "    'location_type',\n",
    "    'location_name',\n",
    "    'closure_periods',\n",
    "    'opening_hours',\n",
    "    'latitude',\n",
    "    'longitude'\n",
    "]\n",
    "\n",
    "# --- 3. Create the final DataFrame for upload with columns in the correct order ---\n",
    "df_for_upload = df[sql_column_order]\n",
    "\n",
    "print(\"DataFrame has been prepared for upload with the correct column order.\")\n",
    "df_for_upload.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160dce53",
   "metadata": {},
   "source": [
    "Insert Data into Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4abcb065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Transaction committed successfully. 244 rows were copied.\n"
     ]
    }
   ],
   "source": [
    "raw_conn = None\n",
    "try:\n",
    "    # Get a single, low-level connection from the engine\n",
    "    raw_conn = engine.raw_connection()\n",
    "    cursor = raw_conn.cursor()\n",
    "\n",
    "    # Set the search path for this session\n",
    "    cursor.execute(\"SET search_path TO berlin_source_data;\")\n",
    "    \n",
    "    # --- Load data into the table ---\n",
    "    # We need to save to the buffer WITH a header for the COPY CSV command to work correctly\n",
    "    buffer = io.StringIO()\n",
    "    df_for_upload.to_csv(buffer, index=False, header=True) # Note: header=True\n",
    "    buffer.seek(0)\n",
    "    \n",
    "    # Use copy_expert with CSV format to correctly handle commas in data\n",
    "    copy_sql = \"\"\"\n",
    "        COPY post_offices FROM STDIN WITH\n",
    "            (FORMAT CSV, HEADER TRUE)\n",
    "    \"\"\"\n",
    "    cursor.copy_expert(sql=copy_sql, file=buffer)\n",
    "    \n",
    "    # Commit the entire transaction\n",
    "    raw_conn.commit()\n",
    "    print(f\"✅ Transaction committed successfully. {len(df_for_upload)} rows were copied.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ An error occurred: {e}\")\n",
    "    if raw_conn:\n",
    "        raw_conn.rollback()\n",
    "finally:\n",
    "    if raw_conn:\n",
    "        raw_conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad37bd0d",
   "metadata": {},
   "source": [
    "Adding the Foreign Key Constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "edb3ea3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Foreign key constraint 'district_id_fk' added successfully.\n"
     ]
    }
   ],
   "source": [
    "# SQL statement to add the foreign key constraint\n",
    "add_foreign_key_sql = \"\"\"\n",
    "ALTER TABLE berlin_source_data.post_offices\n",
    "ADD CONSTRAINT district_id_fk FOREIGN KEY (district_id)\n",
    "REFERENCES berlin_source_data.districts(district_id)\n",
    "ON DELETE RESTRICT\n",
    "ON UPDATE CASCADE;\n",
    "\"\"\"\n",
    "\n",
    "# Connect using the engine and execute the SQL\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        connection.execute(text(add_foreign_key_sql))\n",
    "        connection.commit()\n",
    "    print(\"✅ Foreign key constraint 'district_id_fk' added successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while adding the foreign key: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "82f8f789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Verification: First 5 rows from the database ---\n",
      "        id district_id neighborhood_id zip_code    city                street  \\\n",
      "0  4340626    11001001             101    10178  Berlin        Spandauer Str.   \n",
      "1     6730    11001001             101    10178  Berlin           Rathausstr.   \n",
      "2  4307374    11001001             101    10178  Berlin  Karl-Liebknecht-Str.   \n",
      "3  4125530    11001001             101    10179  Berlin            Grunerstr.   \n",
      "4  4326999    11001001             101    10179  Berlin           Brückenstr.   \n",
      "\n",
      "  house_no            location_type                        location_name  \\\n",
      "0        2            RETAIL_OUTLET                            City Shop   \n",
      "1        5  POSTBANK_FINANCE_CENTER                     Postbank Filiale   \n",
      "2       13            RETAIL_OUTLET                     Lotto Post Tabak   \n",
      "3       20            RETAIL_OUTLET  GECO im ALEXA, Untergeschoss/Baseme   \n",
      "4       1a            RETAIL_OUTLET              Lotto-Post-Schreibwaren   \n",
      "\n",
      "                                     closure_periods  \\\n",
      "0                                                 []   \n",
      "1  [{'type': 'closure', 'fromDate': '2025-10-29T0...   \n",
      "2                                                 []   \n",
      "3                                                 []   \n",
      "4                                                 []   \n",
      "\n",
      "                                       opening_hours   latitude  longitude  \n",
      "0  Mo: 08:00-18:00; Tu: 08:00-18:00; We: 08:00-18...  52.521145  13.403767  \n",
      "1  Mo: 09:30-18:30; Tu: 09:30-18:30; We: 09:30-18...  52.519737  13.411517  \n",
      "2  Mo: 08:00-19:00; Tu: 08:00-19:00; We: 08:00-19...  52.522327  13.408074  \n",
      "3  Mo: 09:00-19:45; Tu: 09:00-19:45; We: 09:00-19...  52.518764  13.416384  \n",
      "4  Mo: 09:00-19:00; Tu: 09:00-19:00; We: 09:00-19...  52.511505  13.416914  \n"
     ]
    }
   ],
   "source": [
    "# Final check: read the first 5 rows from the new table\n",
    "try:\n",
    "    check_df = pd.read_sql(\"SELECT * FROM berlin_source_data.post_offices LIMIT 5\", engine)\n",
    "    print(\"--- Verification: First 5 rows from the database ---\")\n",
    "    print(check_df)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during verification: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11d7fc1",
   "metadata": {},
   "source": [
    "Final Validation via SQL\n",
    "\n",
    "This section executes several SQL queries directly against the database to perform final validation on the newly loaded post_offices_test table. These checks verify the total row count, ensure all coordinates fall within the expected geographical boundaries, and confirm the referential integrity of the district_id by comparing the IDs in the main table against the reference districts table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "69a61d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Total row count in 'post_offices' ---\n",
      "   total_rows\n",
      "0         244\n"
     ]
    }
   ],
   "source": [
    "# --- Query 1: Total row count ---\n",
    "query1 = \"SELECT COUNT(*) AS total_rows FROM berlin_source_data.post_offices;\"\n",
    "df1 = pd.read_sql(query1, engine)\n",
    "print(\"--- 1. Total row count in 'post_offices' ---\")\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d02f0fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2. Count of locations outside Berlin's bounding box ---\n",
      "   outliers\n",
      "0         0\n"
     ]
    }
   ],
   "source": [
    "# --- Query 2: Count of locations with coordinates outside Berlin ---\n",
    "query2 = \"\"\"\n",
    "    SELECT COUNT(*) AS outliers\n",
    "    FROM berlin_source_data.post_offices\n",
    "    WHERE NOT (latitude BETWEEN 52.3 AND 52.7 AND longitude BETWEEN 13.0 AND 13.8);\n",
    "\"\"\"\n",
    "df2 = pd.read_sql(query2, engine)\n",
    "print(\"\\n--- 2. Count of locations outside Berlin's bounding box ---\")\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "76759705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3. Distinct district_id's found in the post_offices table ---\n",
      "   district_id\n",
      "0     11001001\n",
      "1     11002002\n",
      "2     11003003\n",
      "3     11004004\n",
      "4     11005005\n",
      "5     11006006\n",
      "6     11007007\n",
      "7     11008008\n",
      "8     11009009\n",
      "9     11010010\n",
      "10    11011011\n",
      "11    11012012\n"
     ]
    }
   ],
   "source": [
    "# --- Query 3: Distinct district_id's in the post_offices table ---\n",
    "query3 = \"SELECT DISTINCT district_id FROM berlin_source_data.post_offices ORDER BY 1;\"\n",
    "df3 = pd.read_sql(query3, engine)\n",
    "print(\"\\n--- 3. Distinct district_id's found in the post_offices table ---\")\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ac527e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 4. Distinct district_id's from the reference 'districts' table ---\n",
      "   district_id\n",
      "0     11001001\n",
      "1     11002002\n",
      "2     11003003\n",
      "3     11004004\n",
      "4     11005005\n",
      "5     11006006\n",
      "6     11007007\n",
      "7     11008008\n",
      "8     11009009\n",
      "9     11010010\n",
      "10    11011011\n",
      "11    11012012\n"
     ]
    }
   ],
   "source": [
    "# --- Query 4: Distinct district_id's in the districts lookup table ---\n",
    "query4 = \"SELECT DISTINCT district_id FROM berlin_source_data.districts ORDER BY 1;\"\n",
    "df4 = pd.read_sql(query4, engine)\n",
    "print(\"\\n--- 4. Distinct district_id's from the reference 'districts' table ---\")\n",
    "print(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eb9d774f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 5. Number of distinct id in the post_offices table ---\n",
      "   count\n",
      "0    244\n"
     ]
    }
   ],
   "source": [
    "# --- Query 5:  Primary key uniqueness ---\n",
    "query5 = \"SELECT COUNT (DISTINCT id) FROM berlin_source_data.post_offices;\"\n",
    "df5 = pd.read_sql(query5, engine)\n",
    "print(\"\\n--- 5. Number of distinct id in the post_offices table ---\")\n",
    "print(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a09c05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 6. Post offices with an invalid district_id (no match in the districts table) ---\n",
      "Empty DataFrame\n",
      "Columns: [id, district_id]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Check for post offices that have a district_id with no match in the districts table.\n",
    "query6 = \"SELECT po.id, po.district_id FROM berlin_source_data.post_offices po LEFT JOIN berlin_source_data.districts d ON po.district_id = d.district_id WHERE d.district_id IS NULL; -- This finds rows in post_offices that have no match in districts\"\n",
    "df6 = pd.read_sql(query6, engine)\n",
    "print(\"\\n--- 6. Post offices with an invalid district_id (no match in the districts table) ---\")\n",
    "print(df6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64355d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checking the schema of the 'post_offices' table ---\n",
      "        column_name          data_type is_nullable\n",
      "0                id  character varying          NO\n",
      "1       district_id  character varying         YES\n",
      "2   neighborhood_id  character varying         YES\n",
      "3          zip_code  character varying         YES\n",
      "4              city  character varying         YES\n",
      "5            street  character varying         YES\n",
      "6          house_no  character varying         YES\n",
      "7     location_type  character varying         YES\n",
      "8     location_name  character varying         YES\n",
      "9   closure_periods  character varying         YES\n",
      "10    opening_hours  character varying         YES\n",
      "11         latitude            numeric         YES\n",
      "12        longitude            numeric         YES\n"
     ]
    }
   ],
   "source": [
    "# ---  Define the SQL query to get the table schema ---\n",
    "query_schema = \"\"\"\n",
    "SELECT\n",
    "    column_name,\n",
    "    data_type,\n",
    "    is_nullable\n",
    "FROM\n",
    "    information_schema.columns\n",
    "WHERE\n",
    "    table_schema = 'berlin_source_data' AND table_name = 'post_offices';\n",
    "\"\"\"\n",
    "\n",
    "# --- 2. Execute the query and print the result ---\n",
    "try:\n",
    "    print(\"\\n--- Checking the schema of the 'post_offices' table ---\")\n",
    "    \n",
    "    # Execute the query and load the result into a DataFrame\n",
    "    df_schema = pd.read_sql(query_schema, engine)\n",
    "    \n",
    "    # Print the resulting schema information\n",
    "    print(df_schema.to_string())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ An error occurred while executing the query: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1463964c",
   "metadata": {},
   "source": [
    "I'm adding NOT NULL constraints to the required columns in the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93347bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Applying NOT NULL constraints  ---\n",
      "Executing: ALTER TABLE berlin_source_data.post_offices ALTER COLUMN district_id SET NOT NULL;\n",
      "Executing: ALTER TABLE berlin_source_data.post_offices ALTER COLUMN neighborhood_id SET NOT NULL;\n",
      "Executing: ALTER TABLE berlin_source_data.post_offices ALTER COLUMN zip_code SET NOT NULL;\n",
      "Executing: ALTER TABLE berlin_source_data.post_offices ALTER COLUMN city SET NOT NULL;\n",
      "Executing: ALTER TABLE berlin_source_data.post_offices ALTER COLUMN street SET NOT NULL;\n",
      "Executing: ALTER TABLE berlin_source_data.post_offices ALTER COLUMN house_no SET NOT NULL;\n",
      "Executing: ALTER TABLE berlin_source_data.post_offices ALTER COLUMN location_type SET NOT NULL;\n",
      "Executing: ALTER TABLE berlin_source_data.post_offices ALTER COLUMN latitude SET NOT NULL;\n",
      "Executing: ALTER TABLE berlin_source_data.post_offices ALTER COLUMN longitude SET NOT NULL;\n",
      "\n",
      "✅ Constraints applied successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Define the SQL commands to be executed ---\n",
    "sql_commands = [\n",
    "    \"ALTER TABLE berlin_source_data.post_offices ALTER COLUMN district_id SET NOT NULL;\",\n",
    "    \"ALTER TABLE berlin_source_data.post_offices ALTER COLUMN neighborhood_id SET NOT NULL;\",\n",
    "    \"ALTER TABLE berlin_source_data.post_offices ALTER COLUMN zip_code SET NOT NULL;\",\n",
    "    \"ALTER TABLE berlin_source_data.post_offices ALTER COLUMN city SET NOT NULL;\",\n",
    "    \"ALTER TABLE berlin_source_data.post_offices ALTER COLUMN street SET NOT NULL;\",\n",
    "    \"ALTER TABLE berlin_source_data.post_offices ALTER COLUMN house_no SET NOT NULL;\",\n",
    "    \"ALTER TABLE berlin_source_data.post_offices ALTER COLUMN location_type SET NOT NULL;\",\n",
    "    \"ALTER TABLE berlin_source_data.post_offices ALTER COLUMN latitude SET NOT NULL;\",\n",
    "    \"ALTER TABLE berlin_source_data.post_offices ALTER COLUMN longitude SET NOT NULL;\"\n",
    "]   \n",
    "\n",
    "# --- Execute the commands within a transaction ---\n",
    "print(\"\\n--- Applying NOT NULL constraints  ---\")\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        with connection.begin() as transaction:\n",
    "            for command in sql_commands:\n",
    "                print(f\"Executing: {command.strip()}\")\n",
    "                connection.execute(text(command))\n",
    "        \n",
    "        print(\"\\n✅ Constraints applied successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ An error occurred while applying constraints: {e}\")\n",
    "    print(\"\\nNOTE: This error can occur if one of the columns already contains NULL values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3c6d42",
   "metadata": {},
   "source": [
    "Final check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "076972fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checking the schema of the 'post_offices' table ---\n",
      "        column_name          data_type is_nullable\n",
      "0                id  character varying          NO\n",
      "1       district_id  character varying          NO\n",
      "2   neighborhood_id  character varying          NO\n",
      "3          zip_code  character varying          NO\n",
      "4              city  character varying          NO\n",
      "5            street  character varying          NO\n",
      "6          house_no  character varying          NO\n",
      "7     location_type  character varying          NO\n",
      "8     location_name  character varying         YES\n",
      "9   closure_periods  character varying         YES\n",
      "10    opening_hours  character varying         YES\n",
      "11         latitude            numeric          NO\n",
      "12        longitude            numeric          NO\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"\\n--- Checking the schema of the 'post_offices' table ---\")\n",
    "    \n",
    "    # Execute the query and load the result into a DataFrame\n",
    "    df_schema = pd.read_sql(query_schema, engine)\n",
    "    \n",
    "    # Print the resulting schema information\n",
    "    print(df_schema.to_string())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ An error occurred while executing the query: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
