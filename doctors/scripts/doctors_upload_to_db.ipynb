{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8787c9ce",
   "metadata": {},
   "source": [
    "Connecting to the Database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a752e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connection engine for the local 'layereddb' is ready.\n"
     ]
    }
   ],
   "source": [
    "import sqlalchemy as sa\n",
    "\n",
    "# Create the connection string with placeholders for credentials\n",
    "db_uri = \"postgresql+psycopg2://<USERNAME>:<PASSWORD>@localhost:5433/layereddb\"\n",
    "\n",
    "# Create the Engine object, keeping pool_pre_ping for reliability\n",
    "engine = sa.create_engine(db_uri, pool_pre_ping=True)\n",
    "\n",
    "print(\"✅ Connection engine for the local 'layereddb' is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370eedf9",
   "metadata": {},
   "source": [
    "Checking existing schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "56481924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available schemas in the database:\n",
      "['berlin_labels', 'berlin_recommender', 'berlin_source_data', 'dashboard_data', 'information_schema', 'public']\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import inspect\n",
    "# Create an inspector object from engine\n",
    "inspector = inspect(engine)\n",
    "\n",
    "# Get the list of schema names\n",
    "schemas = inspector.get_schema_names()\n",
    "\n",
    "print(\"Available schemas in the database:\")\n",
    "print(schemas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9d7b28",
   "metadata": {},
   "source": [
    "Creating a table 'doctors' in 'berlin_source_data' schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ee585e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to database to create new table 'doctors'...\n",
      "✅ Table 'berlin_source_data.doctors' created successfully.\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import text, create_engine\n",
    "\n",
    "# SQL Statement to Create New Table \n",
    "create_table_sql = \"\"\"\n",
    "-- Drop the table if it already exists to start fresh\n",
    "DROP TABLE IF EXISTS berlin_source_data.doctors;\n",
    "\n",
    "-- Create the new table for doctors and clinics\n",
    "CREATE TABLE berlin_source_data.doctors (\n",
    "    id VARCHAR(30) PRIMARY KEY,\n",
    "    district_id VARCHAR(20) NOT NULL,\n",
    "    neighborhood_id VARCHAR(20) NOT NULL,\n",
    "    name VARCHAR(255),\n",
    "    street VARCHAR(255),\n",
    "    housenumber VARCHAR(30),\n",
    "    city VARCHAR(50),\n",
    "    postcode VARCHAR(10),\n",
    "    amenity VARCHAR(30),\n",
    "    speciality TEXT,\n",
    "    opening_hours VARCHAR(500),\n",
    "    website VARCHAR(500),\n",
    "    longitude DECIMAL(9,6) NOT NULL,\n",
    "    latitude DECIMAL(9,6) NOT NULL,\n",
    "    wheelchair VARCHAR(30),\n",
    "    description TEXT,\n",
    "    email VARCHAR(255),\n",
    "    toilets_wheelchair VARCHAR(30),\n",
    "    wheelchair_description TEXT\n",
    "    \n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Connect and Execute\n",
    "print(\"Connecting to database to create new table 'doctors'...\")\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        # Execute the SQL statement\n",
    "        connection.execute(text(create_table_sql))\n",
    "        \n",
    "        # Commit the transaction\n",
    "        connection.commit()\n",
    "        \n",
    "    print(\"✅ Table 'berlin_source_data.doctors' created successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ An error occurred during table creation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914041e6",
   "metadata": {},
   "source": [
    "Preparing the data for upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "790904ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame 'doctors' has been prepared for upload.\n",
      "Column order now matches the SQL database schema:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1615 entries, 0 to 1614\n",
      "Data columns (total 19 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   id                      1615 non-null   object \n",
      " 1   district_id             1615 non-null   object \n",
      " 2   neighborhood_id         1615 non-null   object \n",
      " 3   name                    1615 non-null   object \n",
      " 4   street                  1611 non-null   object \n",
      " 5   housenumber             1307 non-null   object \n",
      " 6   city                    1615 non-null   object \n",
      " 7   postcode                1545 non-null   object \n",
      " 8   amenity                 1615 non-null   object \n",
      " 9   speciality              1442 non-null   object \n",
      " 10  opening_hours           1116 non-null   object \n",
      " 11  website                 904 non-null    object \n",
      " 12  longitude               1615 non-null   float64\n",
      " 13  latitude                1615 non-null   float64\n",
      " 14  wheelchair              573 non-null    object \n",
      " 15  description             102 non-null    object \n",
      " 16  email                   188 non-null    object \n",
      " 17  toilets_wheelchair      36 non-null     object \n",
      " 18  wheelchair_description  36 non-null     object \n",
      "dtypes: float64(2), object(17)\n",
      "memory usage: 239.9+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import io\n",
    "\n",
    "# Load the CSV file \n",
    "file_to_load = Path('../clean/doctors_clean_with_distr.csv') \n",
    "\n",
    "# Force pandas to read ALL ID columns and 'postcode' as strings (object)\n",
    "df = pd.read_csv(\n",
    "    file_to_load,\n",
    "    dtype={\n",
    "        'id': str, # OSM IDs are sometimes numbers, must be string\n",
    "        'postcode': str,\n",
    "        'district_id': str,\n",
    "        'neighborhood_id': str\n",
    "    }\n",
    ")\n",
    "\n",
    "# Define the correct column order\n",
    "\n",
    "sql_column_order = [\n",
    "    'id',\n",
    "    'district_id',\n",
    "    'neighborhood_id',\n",
    "    'name',\n",
    "    'street',\n",
    "    'housenumber',\n",
    "    'city',\n",
    "    'postcode',\n",
    "    'amenity',\n",
    "    'speciality',\n",
    "    'opening_hours',\n",
    "    'website',\n",
    "    'longitude',\n",
    "    'latitude',\n",
    "    'wheelchair',\n",
    "    'description',\n",
    "    'email',\n",
    "    'toilets_wheelchair',\n",
    "    'wheelchair_description'\n",
    "]\n",
    "\n",
    "# Create the final DataFrame for upload. This re-orders the columns to match the SQL table perfectly\n",
    "df_for_upload = df[sql_column_order]\n",
    "\n",
    "# Check the result\n",
    "print(\"DataFrame 'doctors' has been prepared for upload.\")\n",
    "print(\"Column order now matches the SQL database schema:\")\n",
    "df_for_upload.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babed36f",
   "metadata": {},
   "source": [
    "Insert Data into Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "314f2de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Transaction committed successfully. 1615 rows were copied to 'doctors'.\n"
     ]
    }
   ],
   "source": [
    "raw_conn = None\n",
    "try:\n",
    "    # Get a single, low-level connection from the engine\n",
    "    raw_conn = engine.raw_connection()\n",
    "    cursor = raw_conn.cursor()\n",
    "\n",
    "    # Set the search path for this session\n",
    "    cursor.execute(\"SET search_path TO berlin_source_data;\")\n",
    "    \n",
    "    # Load data into the memory buffer \n",
    "    # We save to the buffer WITH a header (header=True)\n",
    "    buffer = io.StringIO()\n",
    "    df_for_upload.to_csv(buffer, index=False, header=True)\n",
    "    buffer.seek(0) # Reset the buffer to the beginning\n",
    "\n",
    "    # Define the COPY SQL\n",
    "    copy_sql = \"\"\"\n",
    "        COPY doctors FROM STDIN WITH\n",
    "            (FORMAT CSV, HEADER TRUE)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Execute the COPY command\n",
    "    cursor.copy_expert(sql=copy_sql, file=buffer)\n",
    "    \n",
    "    # Commit the entire transaction\n",
    "    raw_conn.commit()\n",
    "    print(f\"✅ Transaction committed successfully. {len(df_for_upload)} rows were copied to 'doctors'.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ An error occurred: {e}\")\n",
    "    if raw_conn:\n",
    "        raw_conn.rollback() # Roll back the transaction on error\n",
    "finally:\n",
    "    if raw_conn:\n",
    "        raw_conn.close() # Always close the raw connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9559645",
   "metadata": {},
   "source": [
    "Adding the Foreign Key Constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "88d44810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Foreign key constraint 'district_id_fk' added successfully.\n"
     ]
    }
   ],
   "source": [
    "# SQL statement to add the foreign key constraint\n",
    "add_foreign_key_sql = \"\"\"\n",
    "ALTER TABLE berlin_source_data.doctors\n",
    "ADD CONSTRAINT district_id_fk FOREIGN KEY (district_id)\n",
    "REFERENCES berlin_source_data.districts(district_id)\n",
    "ON DELETE RESTRICT\n",
    "ON UPDATE CASCADE;\n",
    "\"\"\"\n",
    "\n",
    "# Connect using the engine and execute the SQL\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        connection.execute(text(add_foreign_key_sql))\n",
    "        connection.commit()\n",
    "    print(\"✅ Foreign key constraint 'district_id_fk' added successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while adding the foreign key: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6946ae0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Verification: First 5 rows from the database \n",
      "            id district_id neighborhood_id                              name  \\\n",
      "0   7823742547    11001001             101                  A-Arbeitsmedizin   \n",
      "1  11108705050    11003003             301                 A. Dorosti Nadali   \n",
      "2   4513645689    11002002             201                AID Friedrichshain   \n",
      "3   7844240263    11004004             402  AL Urologie am Rüdesheimer Platz   \n",
      "4   1930491527    11009009             907                               ARZ   \n",
      "\n",
      "                   street housenumber    city postcode   amenity  \\\n",
      "0     Schwartzkopffstraße          15  Berlin    10115  practice   \n",
      "1           Dunckerstraße          17  Berlin    10437  practice   \n",
      "2       Frankfurter Allee         100  Berlin     None  practice   \n",
      "3        Homburger Straße          16  Berlin    14197  practice   \n",
      "4  Albert-Einstein-Straße           4  Berlin    12489  practice   \n",
      "\n",
      "     speciality                                      opening_hours  \\\n",
      "0  occupational                                               None   \n",
      "1       general               Mo,Tu,Th 08:00-18:00; We 08:00-13:00   \n",
      "2          None          Mo-Fr 09:00-13:00; Sa 10:00-12:00; PH off   \n",
      "3       urology  Mo 09:00-13:00,15:00-19:00; Tu 09:00-13:00; We...   \n",
      "4          None                  Mo-Fr 07:30-22:00; Sa 09:00-17:00   \n",
      "\n",
      "                                             website  longitude   latitude  \\\n",
      "0                       https://a-arbeitsmedizin.de/  13.379949  52.535000   \n",
      "1                      https://www.praxis-nadali.de/  13.421853  52.543434   \n",
      "2  https://drogennotdienst.de/angebote/substituti...  13.472960  52.513693   \n",
      "3                        https://www.al-urologie.de/  13.314986  52.474366   \n",
      "4                                               None  13.536666  52.431386   \n",
      "\n",
      "  wheelchair                                     description  \\\n",
      "0       None                                            None   \n",
      "1       None                                            None   \n",
      "2        yes  Substitution und Psychosoziale Betreuung (PSB)   \n",
      "3       None                                            None   \n",
      "4        yes                                            None   \n",
      "\n",
      "                               email toilets_wheelchair wheelchair_description  \n",
      "0           info@a-arbeitsmedizin.de               None                   None  \n",
      "1                               None               None                   None  \n",
      "2  friedrichshain@notdienstberlin.de               None                   None  \n",
      "3                               None               None                   None  \n",
      "4                               None               None                   None  \n"
     ]
    }
   ],
   "source": [
    "# Final check: read the first 5 rows from the new table\n",
    "try:\n",
    "    check_df = pd.read_sql(\"SELECT * FROM berlin_source_data.doctors LIMIT 5\", engine)\n",
    "    print(\" Verification: First 5 rows from the database \")\n",
    "    print(check_df)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during verification: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e35e52",
   "metadata": {},
   "source": [
    "Now we need to load the generated data from healthcare_features.csv into the berlin_labels.district_features table in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "572ad175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update data loaded. Rows: 12\n",
      "Columns to be updated: ['district_id', 'total_primary_adult_score', 'total_pediatric_score', 'specialist_score_total']\n",
      "Role set to data_team.\n",
      "Checked/added column: total_primary_adult_score\n",
      "Checked/added column: total_pediatric_score\n",
      "Checked/added column: specialist_score_total\n",
      "Role reset.\n",
      "Data successfully loaded into staging table berlin_labels.temp_healthcare_scores\n",
      "Granted SELECT permission on berlin_labels.temp_healthcare_scores to data_team.\n",
      "Role set to data_team for update/cleanup.\n",
      "Number of rows updated: 12\n",
      "Role reset.\n",
      "Update operation complete. Staging table dropped.\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "\n",
    "CSV_PATH = '../clean/healthcare_features.csv'\n",
    "TARGET_TABLE = 'berlin_labels.district_features'\n",
    "TEMP_TABLE = 'temp_healthcare_scores' # Name for the temporary staging table\n",
    "SCHEMA_NAME = 'berlin_labels'\n",
    "\n",
    "# Load and Prepare Data \n",
    "try:\n",
    "    df_scores = pd.read_csv(CSV_PATH)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at path {CSV_PATH}\")\n",
    "    # Consider raising an exception or exiting gracefully\n",
    "    raise FileNotFoundError(f\"Could not find the scores file at {CSV_PATH}\")\n",
    "\n",
    "df_scores['district_id'] = df_scores['district_id'].astype(str)\n",
    "\n",
    "# Select only the necessary columns for the update operation\n",
    "UPDATE_COLS = [\n",
    "    'total_primary_adult_score', \n",
    "    'total_pediatric_score', \n",
    "    'specialist_score_total'\n",
    "]\n",
    "df_update = df_scores[['district_id'] + UPDATE_COLS].copy()\n",
    "\n",
    "print(f\"Update data loaded. Rows: {len(df_update)}\")\n",
    "print(f\"Columns to be updated: {df_update.columns.tolist()}\")\n",
    "\n",
    "# Ensure the scores columns are correctly typed (float/decimal) before loading\n",
    "for col in UPDATE_COLS:\n",
    "    df_update[col] = pd.to_numeric(df_update[col], errors='coerce')\n",
    "\n",
    "\n",
    "# Check and Add Columns (ALTER TABLE)\n",
    "# This step ensures the target columns exist in the district_features table.\n",
    "with engine.connect() as connection:\n",
    "    # Set the required role for administrative changes (ALTER TABLE)\n",
    "    connection.execute(text(\"SET ROLE data_team;\")) \n",
    "    print(\"Role set to data_team.\")\n",
    "    \n",
    "    for col in UPDATE_COLS:\n",
    "        try:\n",
    "            # Use DECIMAL for score columns (to match SQL best practice for precise numeric values)\n",
    "            alter_query = text(f\"\"\"\n",
    "                ALTER TABLE {TARGET_TABLE}\n",
    "                ADD COLUMN IF NOT EXISTS \"{col}\" DECIMAL;\n",
    "            \"\"\")\n",
    "            connection.execute(alter_query)\n",
    "            print(f\"Checked/added column: {col}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during ALTER TABLE for {col}: {e}\")\n",
    "            raise # Re-raise the exception to stop the process if critical\n",
    "            \n",
    "    # Reset the role after administrative changes\n",
    "    connection.execute(text(\"RESET ROLE;\"))\n",
    "    print(\"Role reset.\")\n",
    "    \n",
    "    connection.commit()\n",
    "\n",
    "# Load Data into a Temporary Staging Table. We load the dataframe into a temporary table in the same schema.\n",
    "\n",
    "df_update.to_sql(\n",
    "    name=TEMP_TABLE, \n",
    "    con=engine, \n",
    "    if_exists='replace', \n",
    "    index=False,\n",
    "    schema=SCHEMA_NAME\n",
    ")\n",
    "print(f\"Data successfully loaded into staging table {SCHEMA_NAME}.{TEMP_TABLE}\")\n",
    "\n",
    "grant_query = text(f\"\"\"\n",
    "    GRANT SELECT ON TABLE {SCHEMA_NAME}.{TEMP_TABLE} TO data_team;\n",
    "\"\"\")\n",
    "\n",
    "with engine.connect() as grant_connection:\n",
    "    grant_connection.execute(grant_query)\n",
    "    grant_connection.commit()\n",
    "    print(f\"Granted SELECT permission on {SCHEMA_NAME}.{TEMP_TABLE} to data_team.\")\n",
    "\n",
    "# Execute Batch UPDATE (UPSERT) and Cleanup\n",
    "# We use one efficient SQL query to UPDATE the target table based on the data in the staging table, matching by district_id.\n",
    "update_query = text(f\"\"\"\n",
    "    UPDATE {TARGET_TABLE} AS t\n",
    "    SET\n",
    "        total_primary_adult_score = s.total_primary_adult_score,\n",
    "        total_pediatric_score = s.total_pediatric_score,\n",
    "        specialist_score_total = s.specialist_score_total\n",
    "    FROM\n",
    "        {SCHEMA_NAME}.{TEMP_TABLE} AS s\n",
    "    WHERE\n",
    "        t.district_id = s.district_id;\n",
    "\"\"\")\n",
    "\n",
    "drop_query = text(f\"DROP TABLE {SCHEMA_NAME}.{TEMP_TABLE};\")\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    # Set the role again for the UPDATE and DROP operations\n",
    "    connection.execute(text(\"SET ROLE data_team;\"))\n",
    "    print(\"Role set to data_team for update/cleanup.\")\n",
    "    \n",
    "    # Execute the UPDATE\n",
    "    result = connection.execute(update_query)\n",
    "    print(f\"Number of rows updated: {result.rowcount}\")\n",
    "\n",
    "    # Reset the role after changes\n",
    "    connection.execute(text(\"RESET ROLE;\"))\n",
    "    print(\"Role reset.\")\n",
    "    \n",
    "    # Drop the temporary staging table\n",
    "    connection.execute(drop_query)\n",
    "    \n",
    "    connection.commit()\n",
    "    \n",
    "print(\"Update operation complete. Staging table dropped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3854da66",
   "metadata": {},
   "source": [
    "Final Validation via SQL\n",
    "\n",
    "This section executes several SQL queries directly against the database to perform final validation on the newly loaded post_offices_test table. These checks verify the total row count, ensure all coordinates fall within the expected geographical boundaries, and confirm the referential integrity of the district_id by comparing the IDs in the main table against the reference districts table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ab9f64e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total row count in 'doctors'\n",
      "   total_rows\n",
      "0        1615\n"
     ]
    }
   ],
   "source": [
    "# Query 1: Total row count\n",
    "query1 = \"SELECT COUNT(*) AS total_rows FROM berlin_source_data.doctors;\"\n",
    "df1 = pd.read_sql(query1, engine)\n",
    "print(\"Total row count in 'doctors'\")\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5c02e605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Count of locations outside Berlin's bounding box\n",
      "   outliers\n",
      "0         0\n"
     ]
    }
   ],
   "source": [
    "# Query 2: Count of locations with coordinates outside Berlin\n",
    "query2 = \"\"\"\n",
    "    SELECT COUNT(*) AS outliers\n",
    "    FROM berlin_source_data.doctors\n",
    "    WHERE NOT (latitude BETWEEN 52.3 AND 52.7 AND longitude BETWEEN 13.0 AND 13.8);\n",
    "\"\"\"\n",
    "df2 = pd.read_sql(query2, engine)\n",
    "print(\"\\nCount of locations outside Berlin's bounding box\")\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "22270321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Distinct district_id's found in the doctors table\n",
      "   district_id\n",
      "0     11001001\n",
      "1     11002002\n",
      "2     11003003\n",
      "3     11004004\n",
      "4     11005005\n",
      "5     11006006\n",
      "6     11007007\n",
      "7     11008008\n",
      "8     11009009\n",
      "9     11010010\n",
      "10    11011011\n",
      "11    11012012\n"
     ]
    }
   ],
   "source": [
    "# Query 3: Distinct district_id's in the doctors table\n",
    "query3 = \"SELECT DISTINCT district_id FROM berlin_source_data.doctors ORDER BY 1;\"\n",
    "df3 = pd.read_sql(query3, engine)\n",
    "print(\"\\n Distinct district_id's found in the doctors table\")\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "43ba8493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Distinct district_id's from the reference 'districts' table\n",
      "   district_id\n",
      "0     11001001\n",
      "1     11002002\n",
      "2     11003003\n",
      "3     11004004\n",
      "4     11005005\n",
      "5     11006006\n",
      "6     11007007\n",
      "7     11008008\n",
      "8     11009009\n",
      "9     11010010\n",
      "10    11011011\n",
      "11    11012012\n"
     ]
    }
   ],
   "source": [
    "# Query 4: Distinct district_id's in the districts lookup table\n",
    "query4 = \"SELECT DISTINCT district_id FROM berlin_source_data.districts ORDER BY 1;\"\n",
    "df4 = pd.read_sql(query4, engine)\n",
    "print(\"\\n Distinct district_id's from the reference 'districts' table\")\n",
    "print(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9e1610d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Number of distinct id in the doctors table\n",
      "   count\n",
      "0   1615\n"
     ]
    }
   ],
   "source": [
    "# Query 5:  Primary key uniqueness\n",
    "query5 = \"SELECT COUNT (DISTINCT id) FROM berlin_source_data.doctors;\"\n",
    "df5 = pd.read_sql(query5, engine)\n",
    "print(\"\\n Number of distinct id in the doctors table\")\n",
    "print(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "10de5067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Doctors with an invalid district_id (no match in the districts table)\n",
      "Empty DataFrame\n",
      "Columns: [id, district_id]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Query 6: Check for doctors that have a district_id with no match in the districts table.\n",
    "query6 = \"SELECT doс.id, doс.district_id FROM berlin_source_data.doctors doс LEFT JOIN berlin_source_data.districts d ON doс.district_id = d.district_id WHERE d.district_id IS NULL; \"\n",
    "df6 = pd.read_sql(query6, engine)\n",
    "print(\"\\n Doctors with an invalid district_id (no match in the districts table)\")\n",
    "print(df6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4738306a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Starting Post-Load Verification Checks \n",
      "PASS: Rows with new data (12) matches expected staging row count (12).\n",
      "PASS: Data integrity check passed. No unexpected NULLs found in new score columns.\n"
     ]
    }
   ],
   "source": [
    "# Reload df_update to get staging_rows_count (the expected number of updates)\n",
    "try:\n",
    "    df_scores = pd.read_csv(CSV_PATH)\n",
    "except NameError:\n",
    "    # If parameters haven't been defined, we must stop.\n",
    "    print(\"ERROR: Essential parameters (CSV_PATH, etc.) are undefined. Cannot proceed.\")\n",
    "    raise\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at path {CSV_PATH}\")\n",
    "    raise\n",
    "\n",
    "df_update = df_scores.copy() # Used only for getting the expected count\n",
    "staging_rows_count = len(df_update)\n",
    "\n",
    "# POST-LOAD VERIFICATION \n",
    "\n",
    "with engine.connect() as connection:\n",
    "    \n",
    "    # Set the role for reading\n",
    "    connection.execute(text(\"SET ROLE data_team;\")) \n",
    "    \n",
    "    print(\"\\n Starting Post-Load Verification Checks \")\n",
    "    \n",
    "    # Query 1: Get the count of rows in the target table that successfully received data\n",
    "    check_rows_query = text(f\"\"\"\n",
    "        SELECT COUNT(district_id) \n",
    "        FROM {TARGET_TABLE} \n",
    "        WHERE total_primary_adult_score IS NOT NULL;\n",
    "    \"\"\")\n",
    "    \n",
    "    # Execute and retrieve the count of rows that were populated\n",
    "    rows_with_data = connection.execute(check_rows_query).scalar()\n",
    "    \n",
    "    # 1. Comparison: Check if the number of rows populated matches the number of rows expected from the staging data\n",
    "    if rows_with_data == staging_rows_count:\n",
    "        print(f\"PASS: Rows with new data ({rows_with_data}) matches expected staging row count ({staging_rows_count}).\")\n",
    "    else:\n",
    "        print(f\"CRITICAL WARNING: Rows with new data ({rows_with_data}) DOES NOT match expected row count ({staging_rows_count}). Missing or incomplete updates detected.\")\n",
    "\n",
    "    \n",
    "    # Query 2: Check for NULLs in the newly added score columns where data was expected.\n",
    "    check_nulls_query = text(f\"\"\"\n",
    "        SELECT COUNT(district_id)\n",
    "        FROM {TARGET_TABLE}\n",
    "        WHERE \n",
    "            total_primary_adult_score IS NULL \n",
    "            OR total_pediatric_score IS NULL\n",
    "            OR specialist_score_total IS NULL;\n",
    "    \"\"\")\n",
    "\n",
    "    null_count = connection.execute(check_nulls_query).scalar()\n",
    "\n",
    "    # 2. Comparison: Check for data integrity (no unexpected NULLs)\n",
    "    if null_count == 0:\n",
    "        print(\"PASS: Data integrity check passed. No unexpected NULLs found in new score columns.\")\n",
    "    else:\n",
    "        print(f\"CRITICAL ERROR: Data integrity check FAILED. Found {null_count} rows with NULL scores. Investigate the UPDATE operation.\")\n",
    "\n",
    "    \n",
    "    # Reset the role after checking\n",
    "    connection.execute(text(\"RESET ROLE;\"))\n",
    "    \n",
    "    connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d7da46a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking the schema of the 'doctors' table\n",
      "               column_name          data_type is_nullable\n",
      "0                       id  character varying          NO\n",
      "1              district_id  character varying          NO\n",
      "2          neighborhood_id  character varying          NO\n",
      "3                     name  character varying         YES\n",
      "4                   street  character varying         YES\n",
      "5              housenumber  character varying         YES\n",
      "6                     city  character varying         YES\n",
      "7                 postcode  character varying         YES\n",
      "8                  amenity  character varying         YES\n",
      "9               speciality               text         YES\n",
      "10           opening_hours  character varying         YES\n",
      "11                 website  character varying         YES\n",
      "12               longitude            numeric          NO\n",
      "13                latitude            numeric          NO\n",
      "14              wheelchair  character varying         YES\n",
      "15             description               text         YES\n",
      "16                   email  character varying         YES\n",
      "17      toilets_wheelchair  character varying         YES\n",
      "18  wheelchair_description               text         YES\n"
     ]
    }
   ],
   "source": [
    "# Define the SQL query to get the table schema\n",
    "query_schema = \"\"\"\n",
    "SELECT\n",
    "    column_name,\n",
    "    data_type,\n",
    "    is_nullable\n",
    "FROM\n",
    "    information_schema.columns\n",
    "WHERE\n",
    "    table_schema = 'berlin_source_data' AND table_name = 'doctors';\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and print the result\n",
    "try:\n",
    "    print(\"\\nChecking the schema of the 'doctors' table\")\n",
    "    \n",
    "    # Execute the query and load the result into a DataFrame\n",
    "    df_schema = pd.read_sql(query_schema, engine)\n",
    "    \n",
    "    # Print the resulting schema information\n",
    "    print(df_schema.to_string())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ An error occurred while executing the query: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
